{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D48rgmQSB0B6"
   },
   "source": [
    "# Laboratoria 9: BERT i atencja\n",
    "\n",
    "\n",
    "### Zadanie 1 (3 pkt), atencja dekodera względem (en)kodera\n",
    "\n",
    "Poniżej znajdują się dwie macierze, `encoder_states` oraz `decoder_states` reprezentujące stan warstwy ukrytej po przetworzeniu każdego slowa z enkodera i dekodera. Pojedynczy stan warstwy ukrytej zawiera embedding o dlugosci = 3. W enkoderze mamy 4 stany warstwy ukrytej RNNów, gdyż przetwarzamy sekwencję 4 tokenów.\n",
    "\n",
    "W dekoderze mamy 5 tokenów, które powinny być wygenerowane z sekwencji przetwarzanej (en)koderem.\n",
    "\n",
    "Zadanie polega na:\n",
    "a) Obliczniu podobieństwa wszystkich embeddingów z dekodera (queries) względem wszystkich embeddingów kolejnych stanów (en)kodera (keys) [pamiętajcie, że macierze potrafią w transponowanie. W `NumPy` macierz transponujemy za pomocą `macierz.T`]\n",
    "\n",
    "b) Na utworzonej macierzy podobieństwa należy wykonać softmax (zaimportowany z scipy). Uwaga:  pamiętajcie, żeby aplikować softmax w dobrym wymiarze. Wszystkie stany ukryte enkodera powinny zostac zasoftmaksowane względem zadanego stanu dekodera, nie odwrotnie. W scipy, funkcja softmax zawiera argument axis, który może pomóc.\n",
    "\n",
    "c) Należy wykorzystać macierz atencji z kroku b) i `encoder_states` do wygenerowania macierzy zawierającej wektory kontekstu dla każdego tokenu z dekodera.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vQsum9iYATge"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.806   2.376   7.762   1.129]\n",
      " [ 12.164 -12.645  73.935   3.636]\n",
      " [ 27.79  -16.962  94.002   7.137]\n",
      " [ 18.702  -5.184  42.616   4.501]\n",
      " [ 64.38   49.86   56.21   14.45 ]]\n",
      "[[4.91780633e-02 4.32948093e-03 9.45248312e-01 1.24414389e-03]\n",
      " [1.49003187e-27 2.50486173e-38 1.00000000e+00 2.94803216e-31]\n",
      " [1.75587568e-29 6.44090821e-49 1.00000000e+00 1.88369172e-38]\n",
      " [4.11416552e-11 1.74069934e-21 1.00000000e+00 2.79811669e-17]\n",
      " [9.99716568e-01 4.94220792e-07 2.82937800e-04 2.06801368e-22]]\n",
      "[[ 9.69108631  0.35799187  0.59163688]\n",
      " [10.2         0.2         0.3       ]\n",
      " [10.2         0.2         0.3       ]\n",
      " [10.2         0.2         0.3       ]\n",
      " [ 1.20254471  3.39909302  5.59850122]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "# scipy.special.softmax(x, axis=None)\n",
    "\n",
    "encoder_states = np.array(\n",
    "    [[1.2, 3.4, 5.6],    # embedding z warstwy ukrytej enkodera w kroku 1,  np. dla slowa Ala\n",
    "    [-2.3, 0.2, 7.2],   # embedding z warstwy ukrytej enkodera w kroku 2,  np. dla slowa ma\n",
    "    [10.2, 0.2, 0.3],   # embedding z warstwy ukrytej enkodera w kroku 3,  np. dla slowa kota\n",
    "    [0.4, 0.7, 1.2]]    # embedding z warstwy ukrytej enkodera w kroku 4,  np. dla tokenu <EOS> (koniec sekwencji)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "decoder_states = np.array(\n",
    "    [[0.74, 0.23, 0.56],  # embedding z warstwy ukrytej dekodera w kroku 1,  np. przed wygenerowaniem slowa Alice\n",
    "    [7.23, 0.12, 0.55],  # embedding z warstwy ukrytej dekodera w kroku 2,  np. przed wygenerowaniem slowa owns\n",
    "    [9.12, 4.23, 0.44],  # embedding z warstwy ukrytej dekodera w kroku 3,  np. przed wygenerowaniem slowa a\n",
    "    [4.1, 3.23, 0.5],    # embedding z warstwy ukrytej dekodera w kroku 4,  np. przed wygenerowaniem slowa cat\n",
    "    [5.2, 3.1, 8.5]]     # embedding z warstwy ukrytej dekodera w kroku 5,  np. przed wygenerowaniem slowa cat\n",
    ")\n",
    "\n",
    "x = decoder_states.dot(encoder_states.T)\n",
    "x1 = softmax(x, axis=1)\n",
    "x2 = x1.dot(encoder_states)\n",
    "\n",
    "print(x)\n",
    "print(x1)\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwhDwdaTnPku"
   },
   "source": [
    "**Oczekiwane wartości:**\n",
    "\n",
    "a) \n",
    "[[  4.806   2.376   7.762   1.129]\n",
    " [ 12.164 -12.645  73.935   3.636]\n",
    " [ 27.79  -16.962  94.002   7.137]\n",
    " [ 18.702  -5.184  42.616   4.501]\n",
    " [ 64.38   49.86   56.21   14.45 ]] \n",
    "\n",
    "\n",
    "b) \n",
    "[[4.91780633e-02 4.32948093e-03 9.45248312e-01 1.24414389e-03]\n",
    " [1.49003187e-27 2.50486173e-38 1.00000000e+00 2.94803216e-31]\n",
    " [1.75587568e-29 6.44090821e-49 1.00000000e+00 1.88369172e-38]\n",
    " [4.11416552e-11 1.74069934e-21 1.00000000e+00 2.79811669e-17]\n",
    " [9.99716568e-01 4.94220792e-07 2.82937800e-04 2.06801368e-22]] \n",
    "\n",
    "c) \n",
    "[[ 9.69108631  0.35799187  0.59163688]\n",
    " [10.2         0.2         0.3       ]\n",
    " [10.2         0.2         0.3       ]\n",
    " [10.2         0.2         0.3       ]\n",
    " [ 1.20254471  3.39909302  5.59850122]]\n",
    " \n",
    " (albo to samo transponowane)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9220arNHo1V"
   },
   "source": [
    "## Zadanie 2 (2 punkty): tokenizacja tekstu \n",
    "\n",
    "Korzystając z biblioteki transformers (https://huggingface.co/transformers/) wczytaj tokenizator BERTa (BERT to już wytrenowany (pretrenowany) model, oparty o ideę transformera (a w zasadzie o jego enkoder)). Ponieważ model jest gotowy i można go wykorzystać do generowania embeddingów tokenów, ważnym jest, aby tokenizacja była przeprowadzona identycznie do tego jak podczas trenowania BERTa.\n",
    "\n",
    "Wybierzmy pretrenowany tokenizator o nazwie `bert-base-uncased` i zobaczmy jaki będzie efekt tokenizacji na tekście zawartym w zmiennej `text_to_tokenize`.\n",
    "\n",
    "Zwróć uwagę na to, że niektóre rzadkie słowa zostały podzielone na subtokeny -- zgodnie z algorytmem WordPiece jaki omawialiśmy na przedostatnim spotkaniu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yxcU6_pYNgfP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
      "     ---------------------------------------- 4.2/4.2 MB 9.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\miniconda\\envs\\kck\\lib\\site-packages (from transformers) (4.63.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\miniconda\\envs\\kck\\lib\\site-packages (from transformers) (2022.3.2)\n",
      "Requirement already satisfied: requests in d:\\miniconda\\envs\\kck\\lib\\site-packages (from transformers) (2.26.0)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-win_amd64.whl (153 kB)\n",
      "     -------------------------------------- 153.2/153.2 KB 8.9 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
      "     ---------------------------------------- 84.4/84.4 KB 4.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\miniconda\\envs\\kck\\lib\\site-packages (from transformers) (21.0)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp37-cp37m-win_amd64.whl (3.3 MB)\n",
      "     ---------------------------------------- 3.3/3.3 MB 17.4 MB/s eta 0:00:00\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.7.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\miniconda\\envs\\kck\\lib\\site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: importlib-metadata in d:\\miniconda\\envs\\kck\\lib\\site-packages (from transformers) (4.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\miniconda\\envs\\kck\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\miniconda\\envs\\kck\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\andrz\\appdata\\roaming\\python\\python37\\site-packages (from tqdm>=4.27->transformers) (0.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\miniconda\\envs\\kck\\lib\\site-packages (from importlib-metadata->transformers) (3.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\miniconda\\envs\\kck\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\miniconda\\envs\\kck\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\miniconda\\envs\\kck\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\miniconda\\envs\\kck\\lib\\site-packages (from requests->transformers) (1.26.7)\n",
      "Installing collected packages: tokenizers, pyyaml, filelock, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.7.0 huggingface-hub-0.6.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.3; however, version 22.1 is available.\n",
      "You should consider upgrading via the 'D:\\MiniConda\\envs\\kck\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# Uruchom mnie proszę\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uMLn-hE8L6Zh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "text_to_tokenize = \"I've bought a new GPU last year it was GeForce RTX 3060\"\n",
    "tokenizer = None\n",
    "tokens = None\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cOr1m9n6Mw3V"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f0222bbca14a3d8168d6589872b350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdde59189ab3497e88e13ca753c42f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac2592d2de014a48a66fa9744d3b895b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', \"'\", 've', 'bought', 'a', 'new', 'gp', '##u', 'last', 'year', 'it', 'was', 'ge', '##force', 'rt', '##x', '306', '##0']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokens = tokenizer.tokenize(text_to_tokenize)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFwEeX2GOxEp"
   },
   "source": [
    "## Zadanie 3 (brak punktów):\n",
    "Poniżej znajduje się kod wykorzystujący przygotowane wcześniej zmienne `tokenizer` i `tokens` i które dla każdego tokenu z tokens generuje embedding. W odróżnieniu od GloVe, te embeddingi są świadome kontekstu w jakim właśnie występują. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "XHlN4iH8P0Sv"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484d02d5513c4cfb979422b7bc2525eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 768])\n",
      "tensor([ 1.7697e-02, -2.3874e-01,  1.0408e-01,  2.4999e-01, -1.2033e-01,\n",
      "        -4.0158e-01, -9.8396e-01,  7.2941e-01, -2.2074e-01, -4.0561e-01,\n",
      "         1.6345e-01, -1.7978e-01,  3.7820e-01, -6.1495e-02, -5.1600e-01,\n",
      "         4.4331e-02, -3.2295e-01, -3.9708e-02,  2.0338e-01,  1.1511e-01,\n",
      "         6.4235e-01,  1.3435e-01,  3.8067e-01,  4.7144e-01, -1.2596e-01,\n",
      "        -5.3554e-01,  1.6460e-01, -7.6852e-02, -7.3222e-02, -3.4492e-01,\n",
      "         3.4895e-01, -4.5945e-01,  2.2378e-01,  2.0763e-01, -9.3165e-02,\n",
      "        -1.4150e-01,  3.5406e-01, -1.7546e-01, -2.3485e-01,  1.9336e-01,\n",
      "        -6.3215e-01, -6.4281e-01, -2.8872e-01, -8.6114e-02,  1.2085e-03,\n",
      "        -1.1459e-01,  2.9883e-01, -1.6103e-01,  5.3852e-02, -3.5905e-01,\n",
      "        -5.3859e-01, -3.8365e-01, -1.9144e-01, -8.7672e-02,  2.0869e-01,\n",
      "         6.4477e-01,  1.0806e-01, -1.3347e+00, -8.1556e-01,  2.8584e-01,\n",
      "         3.9185e-01, -1.9869e-01, -4.3294e-01,  2.6003e-01,  5.2739e-01,\n",
      "         5.9914e-02, -5.1816e-02,  2.1178e-01,  2.8827e-01, -2.4682e-01,\n",
      "         4.4863e-01, -1.7201e-02, -2.6389e-01,  1.0008e-01,  1.5562e-01,\n",
      "        -1.4528e-01, -1.2204e-01,  4.8833e-01,  3.4538e-01, -1.6892e-01,\n",
      "        -5.5720e-01,  4.8134e-01, -5.6965e-01,  1.8965e-01, -1.1485e-01,\n",
      "        -2.6552e-01, -5.3794e-01,  1.5833e-01, -2.4875e-01, -3.1641e-01,\n",
      "         1.5423e-01, -1.1720e+00,  4.9977e-01,  5.5563e-01,  4.4630e-01,\n",
      "         4.5509e-02, -4.1214e-02,  1.8532e-01, -1.7743e-01,  5.6088e-01,\n",
      "         4.4544e-01, -1.5111e-01, -9.9544e-02, -7.3772e-01, -5.0699e-01,\n",
      "        -2.5241e-01,  4.1852e-01,  3.0720e-01,  7.8197e-02, -4.9396e-02,\n",
      "         1.1951e-01, -6.3064e-03, -2.5727e-01, -4.1277e-01, -3.3628e-01,\n",
      "        -5.1763e-01,  5.4060e-01, -3.6084e-02,  6.5187e-01,  2.3917e-01,\n",
      "        -3.1213e-01, -6.5241e-02,  4.9080e-01,  6.7973e-01,  2.1800e-01,\n",
      "         3.9414e-01, -1.0605e+00,  4.1909e-01, -2.1544e-01, -5.0428e-02,\n",
      "         1.1936e-01,  4.3663e-01,  5.0781e-02, -6.3047e-01,  4.6799e-01,\n",
      "         5.1528e-01, -1.8049e-02, -6.9348e-02, -1.7029e-01, -3.9775e-01,\n",
      "         1.4719e-01,  3.0825e-01,  4.6670e-02,  3.9197e-01, -2.3151e-01,\n",
      "         1.0520e-01, -4.2533e-01, -7.7487e-01, -4.4685e-01, -4.9097e-01,\n",
      "        -3.8706e-01, -5.6053e-02,  5.3618e-01,  4.3031e-01,  9.4330e-02,\n",
      "         2.8188e-01, -9.3716e-01,  1.6683e-01, -4.7589e-01, -1.9005e-02,\n",
      "        -5.2096e-01, -3.5191e-01,  1.1386e-01,  5.2962e-02,  8.2995e-01,\n",
      "        -1.6514e-02, -1.4457e-01,  1.7077e-01,  1.4814e-01, -4.4515e-01,\n",
      "        -4.9155e-01, -1.9570e-01,  7.6810e-01,  1.7237e-01, -1.8192e-01,\n",
      "        -3.3097e-01,  9.8100e-01,  1.2733e-01,  3.3450e-02,  2.9506e-01,\n",
      "        -5.6134e-01,  6.3877e-01, -1.9176e-02,  5.6481e-01,  5.6838e-01,\n",
      "         4.9518e-01,  6.1002e-01,  8.2272e-02,  6.0067e-01,  3.4989e-01,\n",
      "        -1.2023e+00, -3.3540e-01, -2.7095e-01, -3.8797e-02,  4.3411e-01,\n",
      "        -3.7021e-01, -2.3154e-01, -4.2449e-01, -4.4997e-01,  9.4559e-02,\n",
      "         2.8427e-01, -2.8506e-01,  4.8388e-01,  5.2842e-01, -3.3816e-01,\n",
      "        -1.1680e-01, -2.6746e-01,  3.8511e-01,  2.5791e-01,  2.8594e-01,\n",
      "         6.0711e-01,  8.4174e-01,  6.8952e-02,  1.3900e-01, -1.2124e-01,\n",
      "         3.6900e-02, -1.9360e-01, -6.8425e-01,  7.0556e-01,  3.4676e-01,\n",
      "        -1.1494e-01,  4.5076e-03, -3.1707e-01,  3.2416e-01,  3.3213e-01,\n",
      "         3.9318e-01,  5.8333e-01, -2.8541e-01,  5.1213e-02, -3.2785e-01,\n",
      "        -7.3377e-01, -1.0335e+00,  8.8034e-01, -1.1238e-01,  3.7849e-01,\n",
      "         8.5473e-03,  1.8313e-01, -3.3301e-01,  3.1142e-01, -1.9389e-01,\n",
      "         1.4397e-01,  3.6918e-01,  3.3270e-01,  6.1745e-01, -3.9784e-01,\n",
      "         4.8177e-01, -3.1531e-01, -3.6251e-01,  1.0509e-01, -2.3406e-01,\n",
      "         1.2407e-01, -9.8075e-01, -9.4249e-02, -9.1906e-02, -6.8691e-01,\n",
      "        -3.6632e-01, -1.0331e-01,  4.4474e-02,  1.2066e-01,  6.5142e-01,\n",
      "         3.5170e-01,  2.4933e-01, -1.1609e-01, -4.1973e-02, -2.3620e-01,\n",
      "        -4.9580e-01, -6.0369e-01,  5.1362e-01,  2.4923e-01,  3.5410e-01,\n",
      "         1.3835e-01,  2.0582e-01,  1.7806e-02, -5.4401e-02,  3.8541e-01,\n",
      "         2.9347e-01,  1.1412e-01, -1.0834e-01,  2.0766e-02, -3.6135e-01,\n",
      "         1.5473e-01,  3.2106e-01, -5.9579e-02,  2.6053e-01,  2.8379e-01,\n",
      "        -9.9969e-03, -1.4159e-01,  6.8032e-01, -1.9965e-01, -2.0175e-01,\n",
      "         6.4385e-01, -6.2840e-02, -2.9986e-01,  2.7220e-01,  8.8191e-01,\n",
      "         3.5973e-01,  7.7663e-01,  7.2205e-02,  2.4644e-01, -2.3240e-01,\n",
      "        -5.6550e-01,  1.4725e-01,  5.1275e-01,  8.0391e-01,  6.5147e-01,\n",
      "        -2.2750e-01,  3.9503e-01, -1.1022e-01, -5.4596e+00,  6.4147e-02,\n",
      "         2.1879e-01, -2.7906e-01, -3.9831e-02, -3.6300e-01, -1.1635e-01,\n",
      "        -3.4904e-01, -2.3663e-01, -5.3029e-01,  4.8353e-01,  1.1512e-01,\n",
      "         2.8971e-01,  1.0869e-02,  7.4966e-01,  4.0774e-01,  1.0002e-01,\n",
      "        -2.0962e-01, -4.3563e-01,  6.3457e-01,  5.2962e-02,  2.2878e-01,\n",
      "         1.2783e-02, -3.0161e-01,  8.4186e-02,  1.8285e-01,  3.3055e-01,\n",
      "        -3.9347e-01, -7.8684e-02,  1.6808e-02, -3.5441e-01,  8.7663e-02,\n",
      "         1.0883e-01, -3.4643e-03,  9.5432e-02, -2.0548e-01,  3.5901e-01,\n",
      "        -7.3727e-01,  8.9733e-02, -3.3844e-01, -3.1889e-01, -9.2165e-01,\n",
      "         2.0506e-01, -1.8105e-01,  9.4617e-03, -3.2280e-01,  2.9541e-01,\n",
      "        -2.7024e-01, -2.2274e-01,  9.5882e-01,  2.7953e-01,  2.7501e-01,\n",
      "         2.5919e-01, -5.9860e-01,  8.9409e-04,  5.3583e-01,  2.8964e-01,\n",
      "         1.2138e+00, -1.9972e-02,  2.5894e-02, -2.7089e-01,  7.5571e-02,\n",
      "        -8.1176e-01, -4.6658e-01,  4.1829e-01, -2.3441e-01, -2.7985e-01,\n",
      "         2.4811e-01,  7.0093e-02, -3.2193e-01, -6.4252e-01,  3.5622e-01,\n",
      "        -9.8118e-02, -8.7945e-01, -1.0841e-01,  4.3748e-01,  7.6548e-02,\n",
      "        -2.3186e-02, -2.3061e-01, -5.6796e-02, -2.2488e-02, -4.7029e-01,\n",
      "        -1.1985e-01,  1.1921e-01, -7.2712e-01, -1.4211e-01, -1.1862e-01,\n",
      "        -8.3672e-01, -1.0218e-01, -1.7216e-01,  7.4082e-01,  4.2026e-01,\n",
      "         1.1484e-01,  9.0008e-02,  1.1072e+00,  5.7331e-01,  3.7534e-01,\n",
      "        -7.1206e-01,  9.1789e-02, -3.2292e-01,  1.0978e-01,  6.4101e-01,\n",
      "         5.0185e-01, -1.1559e-01,  1.0267e+00, -7.6426e-02, -1.1322e+00,\n",
      "         3.8944e-01,  4.0046e-01,  1.3944e-01, -4.2272e-02, -4.4938e-01,\n",
      "         2.4187e-01, -3.9995e-01, -5.4271e-01, -7.3471e-01,  1.1601e+00,\n",
      "         1.7766e-01, -3.4175e-01, -3.9935e-01, -4.8256e-01,  2.6222e-01,\n",
      "        -2.0135e-01, -8.6668e-01, -5.7130e-01,  3.0100e-01, -5.6240e-01,\n",
      "        -2.0866e-01,  3.1479e-01,  9.1741e-02, -4.1424e-01, -4.0807e-02,\n",
      "        -3.4432e-01,  3.1940e-01, -4.4418e-01, -2.5316e-01, -3.7446e-01,\n",
      "        -6.3915e-01, -1.2812e-01, -6.5513e-02, -5.9778e-01, -1.9546e-01,\n",
      "         2.1890e-01,  3.0434e-01,  4.7342e-01, -1.3671e-01, -3.3395e-01,\n",
      "        -6.1690e-02, -9.1444e-02, -4.8846e-03, -2.7969e-01, -2.3896e-01,\n",
      "         6.2865e-02,  1.4142e-01,  3.2910e-01,  7.5781e-01, -3.7713e-01,\n",
      "        -5.4594e-01, -3.1835e-01, -1.4346e+00,  2.0046e-01,  3.6152e-01,\n",
      "         3.1845e-01,  5.3189e-01,  7.9490e-02,  1.3239e+00,  1.8867e-01,\n",
      "         2.1688e-01,  8.9362e-01, -4.4791e-01, -1.5738e-01, -4.1137e-01,\n",
      "        -3.5896e-01, -2.2947e-01, -5.4256e-01,  1.0867e+00, -2.1479e-02,\n",
      "        -1.5006e-01, -9.6253e-01,  1.1689e-01,  1.8770e-02,  3.2842e-01,\n",
      "        -4.4032e-01,  2.4473e-02,  2.9014e-01,  2.0181e-02,  2.2888e-01,\n",
      "        -3.6310e-01,  1.9150e-01,  4.2221e-01, -5.6120e-01,  5.7717e-01,\n",
      "        -4.5171e-01,  3.1337e-01, -2.8844e-01, -4.8573e-01,  5.1961e-01,\n",
      "        -3.7973e-01, -8.1924e-02, -3.9394e-02,  6.5498e-01,  1.8444e-01,\n",
      "         4.2515e-01,  1.4776e-01, -1.3122e-01, -2.8715e-01,  2.7596e-01,\n",
      "         6.4895e-02,  2.3449e-01,  5.3869e-01, -1.6913e-01, -5.3446e-01,\n",
      "        -9.4972e-02, -4.5731e-02, -3.6774e-01, -7.3747e-01,  5.7373e-01,\n",
      "         3.2957e-01, -5.4817e-01, -4.2633e-02, -2.5472e-01, -3.9164e-01,\n",
      "         4.9192e-01, -3.7632e-01, -1.2553e-01, -5.8869e-01,  2.1538e-01,\n",
      "         2.0356e-01, -3.7334e-01, -6.9017e-01, -1.0857e-01, -6.5877e-01,\n",
      "         3.1972e-01, -1.2121e-01, -4.9353e-01,  7.4761e-01,  1.1735e-01,\n",
      "        -7.9962e-01,  4.2931e-01, -1.3321e-01,  1.4674e-01,  2.9892e-01,\n",
      "        -2.0663e-01, -2.2502e-01, -1.4652e-01,  2.8139e-01, -3.8199e-01,\n",
      "        -1.1545e-02, -8.2582e-01, -2.3883e-02,  3.8041e-01,  4.1206e-01,\n",
      "         2.4404e-01, -2.4122e-01,  2.5008e-02, -4.6492e-01, -8.1239e-01,\n",
      "        -1.0599e-01, -6.1974e-01, -5.6076e-02, -4.9795e-01,  1.6038e-01,\n",
      "        -5.7465e-01, -8.1383e-02,  3.9262e-01,  1.9826e-01,  1.4178e-01,\n",
      "        -1.6296e-01, -3.5966e-01, -4.3727e-01, -2.4156e-01, -4.1971e-01,\n",
      "         2.1356e-02,  1.2074e-01,  4.8065e-01, -1.3105e-01, -6.2449e-01,\n",
      "        -1.9899e-01,  1.1571e+00, -2.9242e-01, -2.5995e-01, -7.8273e-03,\n",
      "         4.5104e-01,  1.0461e-01,  1.6900e-01,  4.1447e-01,  3.0810e-01,\n",
      "         4.5975e-01,  5.3922e-02, -1.1669e-01,  4.2043e-01,  5.9917e-01,\n",
      "         3.3119e-01, -2.8877e-01,  3.1548e-01,  1.5488e-01,  6.5758e-01,\n",
      "         5.3381e-02,  2.2581e-01, -8.3374e-02,  1.8390e-01, -4.8899e-02,\n",
      "         9.7229e-01,  2.6638e-01,  7.6332e-02,  6.8735e-01,  3.2316e-01,\n",
      "        -5.1690e-02,  2.5418e-01, -5.3412e-01,  4.7470e-01, -2.5640e-01,\n",
      "         5.6452e-01,  1.4823e-01, -4.3179e-01, -8.1786e-01,  3.3944e-01,\n",
      "        -1.1436e-01, -1.5774e-01, -2.1686e-01, -5.0589e-01, -3.5801e-01,\n",
      "         4.7428e-01,  1.5342e-01,  2.2372e-01, -2.8166e-01, -2.0519e-01,\n",
      "        -2.2586e-01,  1.0446e-01,  1.7722e-01,  4.3453e-02, -1.8670e-01,\n",
      "         3.8612e-01,  4.0873e-01,  2.4537e-01, -1.3668e-01, -1.9764e-01,\n",
      "        -4.2234e-01,  4.5837e-01,  3.6400e-01, -1.1288e-01,  3.0425e-01,\n",
      "         4.6562e-01,  4.5496e-01,  4.8880e-01,  6.8823e-01,  3.4406e-01,\n",
      "        -2.9347e-01,  7.3627e-01,  2.9985e-01,  1.0709e-01,  1.1006e+00,\n",
      "        -1.9767e-01, -3.3557e-01, -5.4560e-02,  6.3409e-02, -8.5735e-02,\n",
      "         5.5168e-01, -1.0108e-01,  3.4038e-02, -1.8824e-01, -3.3576e-01,\n",
      "        -2.9787e-01, -4.1087e-01, -4.3374e-01,  2.9652e-01,  7.1032e-01,\n",
      "         1.3436e-01, -3.9036e-01, -5.7810e-02,  1.2484e-02,  1.3581e-01,\n",
      "        -2.2797e-01,  2.3689e-01, -1.0999e+00,  1.1243e-01, -6.9562e-01,\n",
      "        -1.1545e+00, -5.4429e-01,  4.4595e-02,  6.5207e-03,  3.9606e-01,\n",
      "        -2.8158e-01, -4.5098e-01, -1.1248e-01, -7.2133e-01, -1.8536e-01,\n",
      "        -3.1471e-01, -5.3526e-01, -5.9247e-01,  1.0692e-01, -2.9659e-01,\n",
      "         4.4186e-01, -2.1612e-01, -1.0636e-01, -8.5082e-01,  1.1968e-01,\n",
      "        -2.3315e-01,  5.6657e-01,  1.2123e-01,  1.6329e-01, -2.3819e-01,\n",
      "        -1.1654e-02,  3.7378e-01,  6.3403e-01,  5.3735e-01,  2.8776e-01,\n",
      "         3.4385e-01, -1.3461e-01,  3.0724e-01,  1.2329e-01,  7.6244e-01,\n",
      "        -4.4161e-01,  5.2202e-01, -6.4517e-01, -1.5274e-02,  6.3411e-01,\n",
      "         8.3228e-01, -6.3035e-01,  8.8902e-02, -1.9873e-01,  2.5249e-01,\n",
      "        -2.9534e-01, -7.8332e-01, -5.5715e-02,  5.2765e-01, -1.5073e-01,\n",
      "         7.5120e-01,  4.3442e-03,  1.1171e-01,  2.6009e-01,  5.9516e-03,\n",
      "        -3.9489e-01,  2.9970e-01, -1.7506e-01,  1.6919e-01, -1.5756e-01,\n",
      "        -4.1416e-01, -1.4509e-01, -1.5062e-01,  5.7134e-01,  2.7012e-01,\n",
      "        -4.5820e-01, -2.7104e-01,  4.9682e-01,  4.3357e-01,  4.0544e-01,\n",
      "        -1.3815e-01, -5.6445e-01, -9.8471e-01, -3.3783e-01, -2.5097e-01,\n",
      "         3.5795e-01, -1.0122e-01,  2.9086e-01, -8.3126e-02, -7.5685e-01,\n",
      "        -3.1325e-01,  1.0305e-01,  9.9203e-02])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "import torch\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased', return_dict=True)  \n",
    "model.eval()  # nie chcemy trenowac modelu, tylko go wykorzystac\n",
    "\n",
    "tokens_with_specials = ['CLS'] + tokens + ['SEP']  # BERT wymaga specjalnych tokenów [CLS] na poczatku i [SEP] separaującego pary zdań (BERT jest trenowany parami zdań)\n",
    "tokens_with_specials = tokenizer.convert_tokens_to_ids(tokens_with_specials)  # zamiana listy tokenow na listę identyfikatorów (liczb) ze slownika\n",
    "tokens_tensor = torch.tensor([tokens_with_specials])  # zamiana na tensor, opakowanie w batch\n",
    "\n",
    "segments = torch.tensor([[1] * len(tokens_with_specials)])  # wygeneruj maskę mówiącą o tym które tokeny nalezą do zdania 1, a ktore do 2. W naszym zadaniu wszystkie tokeny naleza do zdania 1\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, segments)  # wygenerujmy embeddingi BERTem\n",
    "    tokens_embeddings = outputs['last_hidden_state'][0]  # wez pierwszy batch danych i ostatnią warstwę\n",
    "    print(tokens_embeddings.shape)  # 20x768, mamy 20 (sub)tokenów, (18 wlasciwych + cls + sep) i kazdy mapowany jest na wektor liczb o dlugosci 768\n",
    "    print(tokens_embeddings[1])  # wez embedding pierwszego subtokenu z sekwencji (przeskakujemy CLS token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BWi18mj6T_S5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Zadania10-2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
